{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "    # Making sure dimensionless bond length is less than 1\n",
    "    def bond(bl):\n",
    "        return tf.add(K.relu(tf.negative(bl)), K.relu(bl-1.0))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def strength1(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        sorted_porof = K.gather(porof, sortedIndices)\n",
    "        argg = tf.argsort(sorted_bl,axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "        sorted_bl_corr = K.gather(sorted_bl, argg)\n",
    "        return sorted_bl_corr-sorted_bl\n",
    "\n",
    "\n",
    "    def strength2(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        n = K.cast(n, tf.float32)\n",
    "        rel = K.relu(sorted_bl[1:]-sorted_bl[0:-1])\n",
    "        num_vio = K.cast(tf.math.count_nonzero(rel), tf.float32)\n",
    "        return num_vio/n\n",
    "\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam2*K.mean(K.relu(loss3))\n",
    "            return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4*loss4\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam2 * K.mean(K.relu(loss3))\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4 * loss4\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 1\n",
    "        num_epochs = 50\n",
    "        val_frac = 0.2\n",
    "        patience_val = 50\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = \"fine-tuned_\" + pre_train + optimizer_name + '_usePhy' + str(use_YPhy) + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_unique.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_v2.dat')\n",
    "#         x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        x_label = data[:, :-3] # -2 because we do not need porosity predictions\n",
    "        x_labeled = np.hstack((x_label[:,:2],x_label[:,-2:]))\n",
    "        y_labeled = data[:, -3:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = np.hstack((x_unlabeled[:,:2],x_unlabeled[:,-3:-1]))\n",
    "#         x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "#         y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        dependencies = {'root_mean_squared_error': root_mean_squared_error}\n",
    "\n",
    "        # load the pre-trained model using non-calibrated physics-based model predictions (./data/unlabeled.dat)\n",
    "        loaded_model = load_model(results_dir + pre_train, custom_objects=dependencies)\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            model.add(Dropout(rate=drop_frac))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\n",
    "        # pass the weights to all layers but 1st input layer, whose dimensions are updated\n",
    "        for new_layer, layer in zip(model.layers[1:], loaded_model.layers[1:]):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda[0]) # regularization hyper-parameter\n",
    "        lam2 = K.constant(value=lamda[1]) # regularization hyper-parameter\n",
    "        lam3 = K.constant(value=lamda[2]) # regularization hyper-parameter\n",
    "        lam4 = K.constant(value=lamda[3]) # regularization hyper-parameter\n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "    #     porosity = K.relu(predictions[:,1])\n",
    "        phyloss1 = bond(predictions[:,0]) # physics loss 1\n",
    "    #     uinp = K.constant(value=x_unlabeled_non) # unlabeled input data\n",
    "        phyloss2 = poros(init_poro, predictions[:,1]) # physics loss 1\n",
    "        phyloss3 = strength1(predictions[:,0], predictions[:,1])\n",
    "        phyloss4 = strength2(predictions[:,0], predictions[:,1])\n",
    "        totloss = combined_loss([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "        phyloss = phy_loss_mean([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "\n",
    "\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='loss', patience=patience_val, verbose=1)\n",
    "#     history = model.fit(trainX, trainY,\n",
    "#                         batch_size=batch_size,\n",
    "#                         epochs=num_epochs,\n",
    "#                         verbose=1,\n",
    "#                         callbacks=[early_stopping, TerminateOnNaN()])\n",
    "\n",
    "#     test_score = model.evaluate(testX, testY, verbose=0)\n",
    "#     predictions = model.predict(x_labeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     predictions = model.predict(x_unlabeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     print('iter: ' + str(iteration) + ' useYPhy: ' + str(use_YPhy) + \n",
    "#           ' nL: ' + str(n_layers) + ' nN: ' + str(n_nodes) + \n",
    "#           ' lamda1: ' + str(lamda[0]) + ' lamda2: ' + str(lamda[1]) + ' trsize: ' + str(tr_size) + \n",
    "#           ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), ' TestLoss: ' + str(test_score[0]), \"\\n\")\n",
    "\n",
    "# #     print('iter: ' + str(iteration) + ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), \"\\n\")\n",
    "\n",
    "    \n",
    "# #     model.save(model_name)\n",
    "    \n",
    "#     # save results\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'val_loss_1':history.history['val_loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'val_rmse':history.history['val_root_mean_squared_error'],\n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "    \n",
    "\n",
    "#     save_obj(results, results_name)\n",
    "\n",
    "#     predictions = model.predict(testX)\n",
    "#     return results, results_name, predictions, testY, test_score[2], trainY\n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "\n",
    "        Xx = np.random.uniform(0,1,(1000,2))\n",
    "        xx1 = np.ones((1000,2))\n",
    "        Xx = np.hstack((Xx,xx1))\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(int(nsim)):\n",
    "            print(\"simulation num:\",i)\n",
    "            predictions = model.predict(Xx)\n",
    "            predictions = predictions[:,1]\n",
    "            samples.append(predictions[:,np.newaxis])\n",
    "        return np.array(samples)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = 0.1 # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        #set lamda\n",
    "        lamda = [0.3, 0.15, 0.008, 0] # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "        pre_train = 'Pre-trainAdadelta_drop0_nL2_nN5_trsize1308_iter0.h5'\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, use_YPhy, \n",
    "                                               pre_train, tr_size, lamda, iteration, n_nodes, n_layers, drop_frac, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "19/19 [==============================] - 0s 52us/step\n",
      "[0.01662970706820488, 0.00014749089314136654, 0.09520947188138962]\n",
      "simulation num: 0\n"
     ]
    }
   ],
   "source": [
    "pred = pass_arg(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06078615, 0.06992999, 0.05712006, 0.04807645, 0.0516439 ,\n",
       "       0.09402375, 0.03856454, 0.04454408, 0.07344222, 0.05552657,\n",
       "       0.04194411, 0.03320448, 0.04445615, 0.05105675, 0.03610936,\n",
       "       0.06752537, 0.04182122, 0.02382175, 0.04677882, 0.08640023,\n",
       "       0.04145952, 0.03663838, 0.07310407, 0.04578138, 0.04218187,\n",
       "       0.05249367, 0.06594636, 0.02953889, 0.03516436, 0.07315134,\n",
       "       0.04844232, 0.05688644, 0.05055123, 0.03591253, 0.05215218,\n",
       "       0.05950068, 0.03023283, 0.04708679, 0.03862875, 0.05997   ,\n",
       "       0.06490794, 0.06737833, 0.08870946, 0.06987849, 0.04489887,\n",
       "       0.0370475 , 0.06425689, 0.06382453, 0.04631634, 0.06958678,\n",
       "       0.02175514, 0.0359798 , 0.04715684, 0.05161056, 0.04042168,\n",
       "       0.08185203, 0.02543827, 0.04556874, 0.04957148, 0.04591069,\n",
       "       0.07951771, 0.04367394, 0.06938577, 0.06350262, 0.03976859,\n",
       "       0.06043382, 0.01863476, 0.03841164, 0.0602592 , 0.07233427,\n",
       "       0.0463389 , 0.02667804, 0.07340492, 0.09573363, 0.05958167,\n",
       "       0.0430919 , 0.02883442, 0.06046956, 0.08164638, 0.03220722,\n",
       "       0.02430193, 0.0860862 , 0.05841613, 0.04755613, 0.07198708,\n",
       "       0.08496071, 0.07518631, 0.08558965, 0.08712855, 0.04431343,\n",
       "       0.06245499, 0.04061979, 0.05831856, 0.09338179, 0.06826843,\n",
       "       0.06040413, 0.0710997 , 0.0480506 , 0.06657337, 0.03225526,\n",
       "       0.04953476, 0.03098041, 0.0608867 , 0.0462735 , 0.04745485,\n",
       "       0.0408068 , 0.05504208, 0.03469243, 0.06723039, 0.03062294,\n",
       "       0.02707908, 0.0923709 , 0.03830964, 0.04050748, 0.05194235,\n",
       "       0.04018677, 0.06387655, 0.06691667, 0.05443686, 0.05271764,\n",
       "       0.02546912, 0.04791421, 0.11803806, 0.05365252, 0.07977611,\n",
       "       0.04534721, 0.04417   , 0.07517809, 0.10569961, 0.04477115,\n",
       "       0.06489792, 0.0392254 , 0.03911213, 0.04024196, 0.07889178,\n",
       "       0.06323506, 0.08449587, 0.03796664, 0.06847103, 0.0390527 ,\n",
       "       0.07884203, 0.05372062, 0.03801642, 0.04331946, 0.01500129,\n",
       "       0.05618336, 0.03329977, 0.04444757, 0.03669248, 0.11317115,\n",
       "       0.06944309, 0.02555478, 0.05655014, 0.03771107, 0.05064036,\n",
       "       0.03515059, 0.03335508, 0.03121774, 0.03613187, 0.04282602,\n",
       "       0.05417042, 0.05526632, 0.06320415, 0.05442138, 0.06386586,\n",
       "       0.03065534, 0.04388105, 0.0641651 , 0.0929416 , 0.04032328,\n",
       "       0.05841525, 0.03807806, 0.06090528, 0.04523082, 0.03013272,\n",
       "       0.05505478, 0.04485533, 0.0451964 , 0.10702688, 0.06102687,\n",
       "       0.07063188, 0.09552713, 0.06412031, 0.02870737, 0.06457275,\n",
       "       0.04915896, 0.08018278, 0.01644169, 0.07819583, 0.03649619,\n",
       "       0.06267548, 0.07901201, 0.03406535, 0.05255126, 0.05169379,\n",
       "       0.02262175, 0.09385568, 0.04196439, 0.07814562, 0.04634025,\n",
       "       0.02182408, 0.04606622, 0.04131204, 0.0593409 , 0.01735257,\n",
       "       0.01648947, 0.05052439, 0.05562989, 0.07260959, 0.08170186,\n",
       "       0.0465534 , 0.085912  , 0.01802966, 0.10819994, 0.07404017,\n",
       "       0.03734342, 0.0409375 , 0.04229379, 0.05865658, 0.0467818 ,\n",
       "       0.04231539, 0.0624932 , 0.03607972, 0.06691957, 0.03908642,\n",
       "       0.05236423, 0.10390908, 0.05151805, 0.0384759 , 0.01843651,\n",
       "       0.10603432, 0.04858025, 0.04698573, 0.05348984, 0.05677094,\n",
       "       0.06457543, 0.05163852, 0.04653753, 0.05807528, 0.06927304,\n",
       "       0.03516787, 0.12257171, 0.06431985, 0.05282955, 0.05882708,\n",
       "       0.06482004, 0.02707541, 0.03275415, 0.05697012, 0.05174106,\n",
       "       0.11411402, 0.0214011 , 0.04183983, 0.04511038, 0.02833973,\n",
       "       0.05545633, 0.06116821, 0.04436601, 0.06088793, 0.1178361 ,\n",
       "       0.03802024, 0.04001831, 0.04413531, 0.09492186, 0.0527266 ,\n",
       "       0.05387123, 0.04228453, 0.05519046, 0.03946352, 0.08722445,\n",
       "       0.05951046, 0.04502628, 0.10285053, 0.04258338, 0.05321396,\n",
       "       0.04030136, 0.03253463, 0.04427784, 0.05274769, 0.0728232 ,\n",
       "       0.0408493 , 0.04812143, 0.12023468, 0.08807158, 0.03742468,\n",
       "       0.10108665, 0.04826279, 0.06176569, 0.06097072, 0.03701438,\n",
       "       0.05047375, 0.0672247 , 0.06525227, 0.0693609 , 0.05968896,\n",
       "       0.06625161, 0.03423432, 0.06728864, 0.03941203, 0.03944149,\n",
       "       0.04688816, 0.04470814, 0.06823164, 0.09334977, 0.05299336,\n",
       "       0.09899737, 0.04776668, 0.0567801 , 0.04738555, 0.0672413 ,\n",
       "       0.05007158, 0.05871567, 0.03707946, 0.08975832, 0.04048739,\n",
       "       0.06482708, 0.01979798, 0.08313447, 0.05555796, 0.07735252,\n",
       "       0.08278117, 0.06658983, 0.03941642, 0.04914479, 0.09712033,\n",
       "       0.07846054, 0.04464359, 0.0494885 , 0.06666768, 0.03465356,\n",
       "       0.050894  , 0.07049581, 0.04322581, 0.04295136, 0.04358106,\n",
       "       0.05070784, 0.07176584, 0.07330272, 0.06663252, 0.05167495,\n",
       "       0.02916503, 0.0778899 , 0.10634492, 0.05242662, 0.04354152,\n",
       "       0.03664122, 0.04958353, 0.07211736, 0.06936368, 0.04649037,\n",
       "       0.0625264 , 0.06080868, 0.0434542 , 0.07312137, 0.09175569,\n",
       "       0.06823729, 0.06991985, 0.06085801, 0.04243456, 0.04384809,\n",
       "       0.04119788, 0.06939335, 0.09797369, 0.04595125, 0.07318038,\n",
       "       0.09281637, 0.04145097, 0.1122856 , 0.04349801, 0.1077403 ,\n",
       "       0.02581454, 0.04082082, 0.0422891 , 0.02952414, 0.06803387,\n",
       "       0.05483019, 0.05351996, 0.04487892, 0.05208727, 0.02384931,\n",
       "       0.03641276, 0.05794583, 0.04448747, 0.0288039 , 0.07161883,\n",
       "       0.03726837, 0.09982322, 0.01483467, 0.0523807 , 0.04582878,\n",
       "       0.02794747, 0.06968439, 0.04135006, 0.08959362, 0.04703565,\n",
       "       0.06712362, 0.08833691, 0.05354134, 0.0417938 , 0.03006686,\n",
       "       0.03808715, 0.07029289, 0.1110158 , 0.03999357, 0.03724086,\n",
       "       0.03913816, 0.0488643 , 0.05716162, 0.05382175, 0.05501954,\n",
       "       0.02438336, 0.05019951, 0.03211471, 0.11671016, 0.04250966,\n",
       "       0.02618703, 0.05255454, 0.04116978, 0.04560716, 0.05589466,\n",
       "       0.05334563, 0.0893047 , 0.09543715, 0.03599144, 0.05973798,\n",
       "       0.10576834, 0.06573524, 0.05993493, 0.03454085, 0.06785986,\n",
       "       0.05951377, 0.02137998, 0.06148665, 0.07165642, 0.06470925,\n",
       "       0.0293442 , 0.02534739, 0.05753889, 0.04313963, 0.06637323,\n",
       "       0.05827238, 0.04318571, 0.04712432, 0.06529352, 0.07313627,\n",
       "       0.08616604, 0.06904535, 0.03329468, 0.03322306, 0.07184793,\n",
       "       0.03929516, 0.03202866, 0.07242206, 0.0162687 , 0.07038546,\n",
       "       0.06344029, 0.04715439, 0.02085763, 0.08101384, 0.05123076,\n",
       "       0.06587802, 0.0529144 , 0.04920404, 0.0639323 , 0.09541818,\n",
       "       0.048848  , 0.05248797, 0.05250817, 0.09798269, 0.07036749,\n",
       "       0.06196556, 0.04878644, 0.04847172, 0.04390262, 0.05757508,\n",
       "       0.06579557, 0.06104422, 0.03927655, 0.06472339, 0.04553397,\n",
       "       0.05633349, 0.06365986, 0.07627967, 0.04600627, 0.09037575,\n",
       "       0.03533867, 0.05279171, 0.06903802, 0.06629673, 0.05673474,\n",
       "       0.04709506, 0.04430475, 0.03919796, 0.05073019, 0.04663506,\n",
       "       0.04285454, 0.04890167, 0.05250057, 0.04938206, 0.02979896,\n",
       "       0.04103236, 0.03064068, 0.08498178, 0.05895308, 0.052408  ,\n",
       "       0.11112407, 0.071428  , 0.04138963, 0.0645996 , 0.08766183,\n",
       "       0.03321583, 0.05095373, 0.11117247, 0.02270507, 0.04454469,\n",
       "       0.12316932, 0.03614459, 0.03183457, 0.09168795, 0.06263196,\n",
       "       0.05307275, 0.05564339, 0.10270487, 0.07245797, 0.0427365 ,\n",
       "       0.04347289, 0.0388792 , 0.03707257, 0.04172123, 0.05418442,\n",
       "       0.05322568, 0.04756507, 0.08690628, 0.07607311, 0.05907426,\n",
       "       0.04053168, 0.04759775, 0.05178475, 0.10186796, 0.04512649,\n",
       "       0.02317423, 0.03965027, 0.0413128 , 0.06497192, 0.06275481,\n",
       "       0.03494591, 0.03811138, 0.05671794, 0.04477799, 0.09624438,\n",
       "       0.06372042, 0.07207219, 0.06520353, 0.04403849, 0.06517257,\n",
       "       0.04495012, 0.05342287, 0.02107677, 0.09865949, 0.07724486,\n",
       "       0.08834243, 0.04507634, 0.05931891, 0.09051414, 0.06840183,\n",
       "       0.07997648, 0.0716323 , 0.03731409, 0.06149649, 0.09828833,\n",
       "       0.03498491, 0.06851733, 0.05732688, 0.05400645, 0.05817091,\n",
       "       0.05827369, 0.05454987, 0.04615567, 0.04140308, 0.04870323,\n",
       "       0.03935133, 0.07657841, 0.05738584, 0.04126605, 0.07581323,\n",
       "       0.03732901, 0.10941175, 0.11779013, 0.06755163, 0.05431074,\n",
       "       0.02950912, 0.07509418, 0.06469932, 0.04673859, 0.02872391,\n",
       "       0.05887072, 0.04319068, 0.04119461, 0.0652969 , 0.04472395,\n",
       "       0.05474054, 0.04017909, 0.08484197, 0.04523495, 0.05018943,\n",
       "       0.0543396 , 0.05070027, 0.10314739, 0.05420151, 0.01608245,\n",
       "       0.0654245 , 0.07283251, 0.06090727, 0.05444729, 0.10147592,\n",
       "       0.07746957, 0.08222058, 0.05278853, 0.07315373, 0.07135063,\n",
       "       0.0438909 , 0.05136308, 0.05606932, 0.10587737, 0.08405471,\n",
       "       0.04454727, 0.07450049, 0.07561386, 0.05794843, 0.05216443,\n",
       "       0.04864535, 0.06569393, 0.03899365, 0.07651801, 0.05234395,\n",
       "       0.07521193, 0.0469375 , 0.05218047, 0.06561306, 0.05291409,\n",
       "       0.03741595, 0.04552779, 0.03521236, 0.03368699, 0.05576663,\n",
       "       0.01955233, 0.05353649, 0.04515057, 0.04852999, 0.09927544,\n",
       "       0.08028777, 0.03518133, 0.07012595, 0.06512615, 0.03700908,\n",
       "       0.02237308, 0.04369264, 0.03791765, 0.09931593, 0.05265018,\n",
       "       0.07151157, 0.070351  , 0.04521729, 0.05909924, 0.06881595,\n",
       "       0.0527388 , 0.07300232, 0.0578129 , 0.04815583, 0.02889833,\n",
       "       0.0542465 , 0.03747037, 0.10528702, 0.07574508, 0.04019552,\n",
       "       0.07039024, 0.03976347, 0.06830241, 0.055209  , 0.06361537,\n",
       "       0.03544901, 0.03385173, 0.0296802 , 0.06437226, 0.01881116,\n",
       "       0.0516041 , 0.04547605, 0.05554811, 0.02659129, 0.04011157,\n",
       "       0.03207785, 0.04190883, 0.03620005, 0.06340082, 0.02159232,\n",
       "       0.04862833, 0.07494208, 0.04511907, 0.04083867, 0.04643832,\n",
       "       0.07083406, 0.10236417, 0.09085251, 0.07451256, 0.07056217,\n",
       "       0.06306088, 0.02418024, 0.0691445 , 0.07964811, 0.05795467,\n",
       "       0.06114402, 0.11840898, 0.06217865, 0.04977372, 0.06430352,\n",
       "       0.07348585, 0.0744234 , 0.0611016 , 0.04000393, 0.05257339,\n",
       "       0.0515723 , 0.03997387, 0.03238834, 0.04818501, 0.0479946 ,\n",
       "       0.04855963, 0.08746   , 0.05783961, 0.04948503, 0.02771008,\n",
       "       0.06504913, 0.04571201, 0.05615178, 0.04530248, 0.04946012,\n",
       "       0.05168187, 0.05238439, 0.06180153, 0.02523606, 0.06529948,\n",
       "       0.03255951, 0.05690957, 0.03991385, 0.0500801 , 0.07887653,\n",
       "       0.0779368 , 0.10101648, 0.02512636, 0.06443493, 0.04568388,\n",
       "       0.07534459, 0.09848346, 0.05053646, 0.03897728, 0.03535319,\n",
       "       0.029632  , 0.06859353, 0.04795242, 0.04830193, 0.05957587,\n",
       "       0.05570793, 0.04813966, 0.06275256, 0.04322575, 0.09556951,\n",
       "       0.06063207, 0.06723337, 0.05735602, 0.04704462, 0.03866122,\n",
       "       0.06059402, 0.08472998, 0.06096775, 0.01529021, 0.04393271,\n",
       "       0.0376094 , 0.06012197, 0.06677066, 0.03768812, 0.0223082 ,\n",
       "       0.04504803, 0.04694362, 0.04224483, 0.0779461 , 0.07800008,\n",
       "       0.05482635, 0.04431121, 0.07508147, 0.07779223, 0.05337783,\n",
       "       0.07456015, 0.04305593, 0.06373714, 0.11216336, 0.04810935,\n",
       "       0.05278946, 0.04096764, 0.05828026, 0.01986088, 0.05949856,\n",
       "       0.04185868, 0.03501706, 0.09584737, 0.0462524 , 0.04363828,\n",
       "       0.06931716, 0.03960133, 0.04792802, 0.05381459, 0.05731399,\n",
       "       0.07442051, 0.10297367, 0.04632934, 0.02269448, 0.10340135,\n",
       "       0.06343997, 0.0589257 , 0.05650373, 0.12004942, 0.04046642,\n",
       "       0.10416521, 0.05520326, 0.05011043, 0.04079708, 0.03685634,\n",
       "       0.0549824 , 0.06363225, 0.03555796, 0.06297946, 0.04406596,\n",
       "       0.04302999, 0.04266923, 0.05967611, 0.04417478, 0.04344762,\n",
       "       0.0189211 , 0.03173318, 0.06341642, 0.10130593, 0.04246975,\n",
       "       0.09079434, 0.05681304, 0.05331497, 0.06112777, 0.05964226,\n",
       "       0.04514987, 0.04831601, 0.0561516 , 0.05249648, 0.07306956,\n",
       "       0.0356523 , 0.04000038, 0.11781171, 0.03874538, 0.05233026,\n",
       "       0.08699869, 0.05328333, 0.07578385, 0.04115591, 0.04725307,\n",
       "       0.04574725, 0.0374154 , 0.0594375 , 0.0369441 , 0.03872779,\n",
       "       0.06142494, 0.05442145, 0.07995388, 0.06218081, 0.02227316,\n",
       "       0.08766139, 0.07919629, 0.06646134, 0.04649846, 0.03527915,\n",
       "       0.05170125, 0.05878818, 0.04560442, 0.07028557, 0.03780883,\n",
       "       0.05075743, 0.05046223, 0.03477488, 0.04196141, 0.03015474,\n",
       "       0.0607686 , 0.03385814, 0.04213446, 0.04098247, 0.0343647 ,\n",
       "       0.04727486, 0.09022579, 0.06568812, 0.09925036, 0.05991669,\n",
       "       0.06018915, 0.05625014, 0.09730683, 0.05082672, 0.01537219,\n",
       "       0.04667853, 0.0480424 , 0.03718136, 0.03925277, 0.05796484,\n",
       "       0.07077935, 0.04357503, 0.06431355, 0.08674569, 0.05436855,\n",
       "       0.06115055, 0.03766764, 0.04725249, 0.0639872 , 0.06354393,\n",
       "       0.07675306, 0.05040784, 0.03624888, 0.05665757, 0.07031207,\n",
       "       0.03742009, 0.04559456, 0.05859735, 0.06380689, 0.07083619,\n",
       "       0.11200258, 0.05990608, 0.07794058, 0.10250512, 0.02294838,\n",
       "       0.09264384, 0.05426706, 0.04223178, 0.0479462 , 0.02782531,\n",
       "       0.07996017, 0.04685714, 0.05317352, 0.05217699, 0.07928726,\n",
       "       0.06968586, 0.05427977, 0.03909369, 0.10229833, 0.06470831,\n",
       "       0.04293504, 0.10432141, 0.07423103, 0.04623269, 0.04791534,\n",
       "       0.09609066, 0.03038615, 0.03272915, 0.04179104, 0.03710365,\n",
       "       0.04939495, 0.07469167, 0.05904778, 0.10664424, 0.04536283,\n",
       "       0.0551961 , 0.05566236, 0.0224759 , 0.06396493, 0.07673766,\n",
       "       0.02468054, 0.07812186, 0.04305054, 0.04544378, 0.07480203,\n",
       "       0.03730855, 0.05960806, 0.06090723, 0.04318236, 0.07149152,\n",
       "       0.03664266, 0.0447927 , 0.04110497, 0.04431675, 0.06710723,\n",
       "       0.03921742, 0.02563598, 0.04495014, 0.01892487, 0.04283132,\n",
       "       0.10704   , 0.05988109, 0.05758693, 0.03102348, 0.07144113,\n",
       "       0.02109479, 0.04610196, 0.04967417, 0.05351326, 0.10347641,\n",
       "       0.04048614, 0.06876232, 0.01957841, 0.04471598, 0.03247602],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(pred, \"../pred_upd_loss_hyb_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
