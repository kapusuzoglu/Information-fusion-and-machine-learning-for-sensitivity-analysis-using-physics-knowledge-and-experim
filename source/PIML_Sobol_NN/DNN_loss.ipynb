{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adadelta, Adagrad, Adam, Nadam, SGD\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "from keras.regularizers import l1_l2\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # def save_obj(obj, name):\n",
    "    #     with open(name, 'wb') as f:\n",
    "    #         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "    # Making sure dimensionless bond length is less than 1\n",
    "    def bond(bl):\n",
    "        return tf.add(K.relu(tf.negative(bl)), K.relu(bl-1.0))\n",
    "\n",
    "    # Making sure final porosity is less than initial\n",
    "    def poros(poroi, porof):\n",
    "        return K.relu(tf.negative(porof)) + K.relu(porof-poroi)\n",
    "\n",
    "    def strength1(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        sorted_porof = K.gather(porof, sortedIndices)\n",
    "        argg = tf.argsort(sorted_bl,axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "        sorted_bl_corr = K.gather(sorted_bl, argg)\n",
    "        return sorted_bl_corr-sorted_bl\n",
    "\n",
    "\n",
    "    def strength2(bl, porof, nlayer=6):\n",
    "        sigma01, sigma02, C1s = 6, 31, 21\n",
    "        sigma_long = sigma01*(K.exp((1.0-porof)**(C1s*nlayer))-porof) + sigma02*(1.0-porof)\n",
    "        n = K.shape(sigma_long)[0]  \n",
    "        sorted_strength, sortedIndices = tf.math.top_k(sigma_long, n, True)\n",
    "        sorted_bl = K.gather(bl, sortedIndices)\n",
    "        n = K.cast(n, tf.float32)\n",
    "        rel = K.relu(sorted_bl[1:]-sorted_bl[0:-1])\n",
    "        num_vio = K.cast(tf.math.count_nonzero(rel), tf.float32)\n",
    "        return num_vio/n\n",
    "\n",
    "\n",
    "    def phy_loss_mean(params):\n",
    "        # useful for cross-checking training\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam2*K.mean(K.relu(loss3))\n",
    "            return lam1*K.mean(K.relu(loss1)) + lam2*K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4*loss4\n",
    "        return loss\n",
    "\n",
    "    #function to calculate the combined loss = sum of rmse and phy based loss\n",
    "    def combined_loss(params):\n",
    "        loss1, loss2, loss3, loss4, lam1, lam2, lam3, lam4 = params\n",
    "        def loss(y_true,y_pred):\n",
    "    #         return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam2 * K.mean(K.relu(loss3))\n",
    "            return mean_squared_error(y_true, y_pred) + lam1 * K.mean(K.relu(loss1)) + lam2 * K.mean(K.relu(loss2)) + lam3*K.mean(K.relu(loss3)) + lam4 * loss4\n",
    "        return loss\n",
    "\n",
    "    def PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, iteration, n_layers, n_nodes, tr_size, lamda, reg, samp):\n",
    "\n",
    "    #     fix_seeds(ss)\n",
    "\n",
    "        # Hyper-parameters of the training process\n",
    "    #     batch_size = tr_size\n",
    "        batch_size = 1\n",
    "        num_epochs = 50\n",
    "        val_frac = 0.2\n",
    "        patience_val = 50\n",
    "\n",
    "        # Initializing results filename\n",
    "        exp_name = \"DNN_loss\" + optimizer_name + '_drop' + str(drop_frac) + '_usePhy' + str(use_YPhy) +  '_nL' + str(n_layers) + '_nN' + str(n_nodes) + '_trsize' + str(tr_size) + '_lamda' + str(lamda) + '_iter' + str(iteration)\n",
    "        exp_name = exp_name.replace('.','pt')\n",
    "        results_dir = '../results/'\n",
    "        model_name = results_dir + exp_name + '_model.h5' # storing the trained model\n",
    "\n",
    "        if reg==True and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==25:\n",
    "            results_name = results_dir + exp_name + '_results_25.dat' # storing the results of the model\n",
    "        elif reg==True and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519_regularizer.dat' # storing the results of the model\n",
    "        elif reg==False and samp==1519:\n",
    "            results_name = results_dir + exp_name + '_results_1519.dat' # storing the results of the model\n",
    "\n",
    "        # Load labeled data\n",
    "        data = np.loadtxt('../data/labeled_data.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_unique.dat')\n",
    "    #     data = np.loadtxt('../data/labeled_data_BK_constw_v2.dat')\n",
    "        x_labeled = data[:, :2] # -2 because we do not need porosity predictions\n",
    "        y_labeled = data[:, -3:-1] # dimensionless bond length and porosity measurements\n",
    "        if samp==25:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_25.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "        elif samp==1519:\n",
    "            data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "            x_unlabeled = data[:, :]\n",
    "\n",
    "        x_unlabeled1 = x_unlabeled[:1303, :]\n",
    "        x_unlabeled2 = x_unlabeled[-6:, :]\n",
    "        x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "        # initial porosity\n",
    "        init_poro = x_unlabeled[:, -1]\n",
    "        x_unlabeled = x_unlabeled[:, :2]\n",
    "\n",
    "    #     data = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1519.dat')\n",
    "    #     x_unlabeled = data[:1303, :] # 1303 last regular sample: 260, 46\n",
    "    #     x_unlabeled_non = x_unlabeled\n",
    "\n",
    "\n",
    "\n",
    "        # normalize dataset with MinMaxScaler\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    #     scaler = preprocessing.StandardScaler()\n",
    "        x_labeled = scaler.fit_transform(x_labeled)\n",
    "        x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "#         y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    #     # initial porosity & physics outputs are removed\n",
    "    #     x_unlabeled = x_unlabeled[:, :-3]\n",
    "\n",
    "        # train and test data\n",
    "        trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    #     testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]   \n",
    "        testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "        if use_YPhy == 0:\n",
    "            # Removing the last column from x_unlabeled (corresponding to Y_PHY)\n",
    "            x_unlabeled = x_unlabeled[:,:-1]\n",
    "\n",
    "        # Creating the model\n",
    "        model = Sequential()\n",
    "        for layer in np.arange(n_layers):\n",
    "            if layer == 0:\n",
    "                model.add(Dense(n_nodes, activation='relu', input_shape=(np.shape(trainX)[1],)))\n",
    "            else:\n",
    "                if reg:\n",
    "                    model.add(Dense(n_nodes, activation='relu', kernel_regularizer=l1_l2(l1=.001, l2=.001)))\n",
    "                else:\n",
    "                    model.add(Dense(n_nodes, activation='relu'))\n",
    "            model.add(Dropout(rate=drop_frac))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "\n",
    "        # physics-based regularization\n",
    "        uinp_sc = K.constant(value=x_unlabeled) # unlabeled input data\n",
    "        lam1 = K.constant(value=lamda[0]) # regularization hyper-parameter\n",
    "        lam2 = K.constant(value=lamda[1]) # regularization hyper-parameter\n",
    "        lam3 = K.constant(value=lamda[2]) # regularization hyper-parameter\n",
    "        lam4 = K.constant(value=lamda[3]) # regularization hyper-parameter\n",
    "        predictions = model(uinp_sc) # model output at depth i\n",
    "    #     porosity = K.relu(predictions[:,1])\n",
    "        phyloss1 = bond(predictions[:,0]) # physics loss 1\n",
    "    #     uinp = K.constant(value=x_unlabeled_non) # unlabeled input data\n",
    "        phyloss2 = poros(init_poro, predictions[:,1]) # physics loss 1\n",
    "        phyloss3 = strength1(predictions[:,0], predictions[:,1])\n",
    "        phyloss4 = strength2(predictions[:,0], predictions[:,1])\n",
    "        totloss = combined_loss([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "        phyloss = phy_loss_mean([phyloss1, phyloss2, phyloss3, phyloss4, lam1, lam2, lam3, lam4])\n",
    "\n",
    "\n",
    "        model.compile(loss=totloss,\n",
    "                      optimizer=optimizer_val,\n",
    "                      metrics=[phyloss, root_mean_squared_error])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience_val, verbose=1)\n",
    "\n",
    "    #     print('Running...' + optimizer_name)\n",
    "        history = model.fit(trainX, trainY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=num_epochs,\n",
    "                            verbose=0,\n",
    "                            validation_split=val_frac, callbacks=[early_stopping, TerminateOnNaN()])\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='loss', patience=patience_val, verbose=1)\n",
    "#     history = model.fit(trainX, trainY,\n",
    "#                         batch_size=batch_size,\n",
    "#                         epochs=num_epochs,\n",
    "#                         verbose=1,\n",
    "#                         callbacks=[early_stopping, TerminateOnNaN()])\n",
    "\n",
    "#     test_score = model.evaluate(testX, testY, verbose=0)\n",
    "#     predictions = model.predict(x_labeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     predictions = model.predict(x_unlabeled) # model output at depth i\n",
    "#     print(np.sort(predictions[:,0], axis=0))\n",
    "    \n",
    "#     print('iter: ' + str(iteration) + ' useYPhy: ' + str(use_YPhy) + \n",
    "#           ' nL: ' + str(n_layers) + ' nN: ' + str(n_nodes) + \n",
    "#           ' lamda1: ' + str(lamda[0]) + ' lamda2: ' + str(lamda[1]) + ' trsize: ' + str(tr_size) + \n",
    "#           ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), ' TestLoss: ' + str(test_score[0]), \"\\n\")\n",
    "\n",
    "# #     print('iter: ' + str(iteration) + ' TestRMSE: ' + str(test_score[2]) + ' PhyLoss: ' + str(test_score[1]), \"\\n\")\n",
    "\n",
    "    \n",
    "# #     model.save(model_name)\n",
    "    \n",
    "#     # save results\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'val_loss_1':history.history['val_loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'val_rmse':history.history['val_root_mean_squared_error'],\n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "#     results = {'train_loss_1':history.history['loss_1'], \n",
    "#                                 'train_rmse':history.history['root_mean_squared_error'], \n",
    "#                                 'test_rmse':test_score[2],\n",
    "#                                 'PhyLoss':test_score[1]}\n",
    "\n",
    "    \n",
    "\n",
    "#     save_obj(results, results_name)\n",
    "\n",
    "#     predictions = model.predict(testX)\n",
    "#     return results, results_name, predictions, testY, test_score[2], trainY\n",
    "    \n",
    "        test_score = model.evaluate(testX, testY, verbose=1)\n",
    "        print(test_score)\n",
    "        \n",
    "        Xx = np.random.uniform(0,1,(1000,2))\n",
    "\n",
    "        samples = []\n",
    "        for i in range(int(nsim)):\n",
    "            print(\"simulation num:\",i)\n",
    "            predictions = model.predict(Xx)\n",
    "            predictions = predictions[:,1]\n",
    "            samples.append(predictions[:,np.newaxis])\n",
    "        return np.array(samples)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Main Function\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        fix_seeds(1)\n",
    "\n",
    "        # List of optimizers to choose from    \n",
    "        optimizer_names = ['Adagrad', 'Adadelta', 'Adam', 'Nadam', 'RMSprop', 'SGD', 'NSGD']\n",
    "        optimizer_vals = [Adagrad(clipnorm=1), Adadelta(clipnorm=1), Adam(clipnorm=1), Nadam(clipnorm=1), RMSprop(clipnorm=1), SGD(clipnorm=1.), SGD(clipnorm=1, nesterov=True)]\n",
    "\n",
    "        # selecting the optimizer\n",
    "        optimizer_num = 1\n",
    "        optimizer_name = optimizer_names[optimizer_num]\n",
    "        optimizer_val = optimizer_vals[optimizer_num]\n",
    "\n",
    "        # Selecting Other Hyper-parameters\n",
    "        drop_frac = 0.1 # Fraction of nodes to be dropped out\n",
    "        use_YPhy = 1 # Whether YPhy is used as another feature in the NN model or not\n",
    "        n_layers = 2 # Number of hidden layers\n",
    "        n_nodes = 5 # Number of nodes per hidden layer\n",
    "\n",
    "        #set lamda\n",
    "        lamda = [0.3, 0.15, 0.008, 0] # Physics-based regularization constant  \n",
    "\n",
    "#         # Iterating over different training fractions and splitting indices for train-test splits\n",
    "#         trsize_range = [4,6,8,10,20]\n",
    "\n",
    "#         #default training size = 5000\n",
    "#         tr_size = trsize_range[4]\n",
    "        \n",
    "        tr_size = int(tr_size)\n",
    "\n",
    "        # use regularizer\n",
    "        reg = True\n",
    "\n",
    "        # sample size used\n",
    "        samp = 1519\n",
    "    #     samp = 25\n",
    "\n",
    "        # total number of runs\n",
    "        iter_range = np.arange(1)\n",
    "        testrmse=[]\n",
    "        # iterating through all possible params\n",
    "        for iteration in iter_range:\n",
    "#             results, result_file, pred, obs, rmse, obs_train = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "#                             iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "#             testrmse.append(rmse)\n",
    "            pred = PGNN_train_test(optimizer_name, optimizer_val, drop_frac, use_YPhy, \n",
    "                            iteration, n_layers, n_nodes, tr_size, lamda, reg, samp)\n",
    "            \n",
    "\n",
    "    return np.squeeze(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr_size: 20\n",
      "19/19 [==============================] - 0s 0us/step\n",
      "[0.007046337705105543, 0.00012185268860775977, 0.04214569181203842]\n",
      "simulation num: 0\n"
     ]
    }
   ],
   "source": [
    "pred = pass_arg(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04840159, 0.03267747, 0.04884847, 0.03874322, 0.03060409,\n",
       "       0.0393777 , 0.03722005, 0.04372139, 0.03305069, 0.04452587,\n",
       "       0.05805647, 0.04023119, 0.03662075, 0.02928954, 0.0387533 ,\n",
       "       0.03258938, 0.04021182, 0.04368255, 0.03258711, 0.04443935,\n",
       "       0.06012994, 0.04745853, 0.03669268, 0.03393222, 0.06270187,\n",
       "       0.04490576, 0.03079578, 0.04299763, 0.04479793, 0.04918788,\n",
       "       0.03291145, 0.03778812, 0.03652166, 0.05324904, 0.028528  ,\n",
       "       0.04281557, 0.05556521, 0.03065464, 0.06921013, 0.03197813,\n",
       "       0.0468891 , 0.03143737, 0.03327765, 0.03588666, 0.05554689,\n",
       "       0.05631546, 0.04064337, 0.04547366, 0.04335863, 0.03438437,\n",
       "       0.04966576, 0.04260882, 0.03612067, 0.03854731, 0.06650812,\n",
       "       0.04749881, 0.04423252, 0.05128912, 0.0440781 , 0.03345554,\n",
       "       0.03373761, 0.03416363, 0.0379544 , 0.0369825 , 0.06180551,\n",
       "       0.04209778, 0.0426929 , 0.05823708, 0.03411588, 0.03189576,\n",
       "       0.03966862, 0.04676951, 0.04665989, 0.03774998, 0.03002513,\n",
       "       0.04623103, 0.04187569, 0.03086422, 0.03841453, 0.04062474,\n",
       "       0.04217795, 0.03570224, 0.04211181, 0.0483807 , 0.03373912,\n",
       "       0.043412  , 0.03310084, 0.03921049, 0.0328992 , 0.04207412,\n",
       "       0.04062744, 0.04035528, 0.0415933 , 0.03848179, 0.05011772,\n",
       "       0.03080095, 0.03839112, 0.04804714, 0.03037823, 0.05031215,\n",
       "       0.04879351, 0.04898253, 0.0344385 , 0.05122439, 0.03654142,\n",
       "       0.06420502, 0.04577678, 0.03837265, 0.04798601, 0.04323298,\n",
       "       0.04261087, 0.03603601, 0.06180106, 0.0638002 , 0.0369878 ,\n",
       "       0.04249903, 0.03165926, 0.03640934, 0.04014939, 0.05846129,\n",
       "       0.04322065, 0.05236199, 0.03049233, 0.0417861 , 0.04182184,\n",
       "       0.03896416, 0.05654829, 0.0315931 , 0.03722509, 0.03398994,\n",
       "       0.03849557, 0.06061681, 0.05219116, 0.05996954, 0.04419868,\n",
       "       0.03815373, 0.03745332, 0.04133959, 0.0386733 , 0.05862823,\n",
       "       0.04535113, 0.02986287, 0.03632551, 0.03372463, 0.04331812,\n",
       "       0.0498048 , 0.05657608, 0.05504951, 0.0495047 , 0.03514851,\n",
       "       0.03344829, 0.04346341, 0.04186874, 0.04212542, 0.03983863,\n",
       "       0.05319478, 0.0490943 , 0.0531509 , 0.03941287, 0.04164888,\n",
       "       0.04719696, 0.03015653, 0.03978382, 0.03963805, 0.0309941 ,\n",
       "       0.05472104, 0.03970988, 0.0434128 , 0.0364522 , 0.03650068,\n",
       "       0.04516755, 0.06253941, 0.04002846, 0.05305981, 0.04249009,\n",
       "       0.04200388, 0.05313521, 0.03173988, 0.02428573, 0.03055502,\n",
       "       0.03883207, 0.03934572, 0.04167803, 0.05324339, 0.03586891,\n",
       "       0.04965905, 0.03750531, 0.04292093, 0.03148001, 0.06250868,\n",
       "       0.04009802, 0.03350842, 0.05667091, 0.03934377, 0.04312398,\n",
       "       0.04348752, 0.03504463, 0.06026124, 0.03368675, 0.03498913,\n",
       "       0.04326471, 0.06965242, 0.06361095, 0.03892917, 0.04267493,\n",
       "       0.04286293, 0.02882685, 0.03734808, 0.03751354, 0.03403379,\n",
       "       0.04396749, 0.04661652, 0.04358124, 0.03081626, 0.03716754,\n",
       "       0.05368581, 0.04265843, 0.06008489, 0.03282315, 0.04775698,\n",
       "       0.04314178, 0.03507447, 0.05670633, 0.04111083, 0.05304705,\n",
       "       0.046885  , 0.03951255, 0.04073916, 0.04012356, 0.04198017,\n",
       "       0.02492835, 0.031758  , 0.03295476, 0.04467816, 0.03553396,\n",
       "       0.04956528, 0.0448807 , 0.04015817, 0.0454204 , 0.03313018,\n",
       "       0.04183149, 0.01902162, 0.04303076, 0.03413093, 0.03212382,\n",
       "       0.03415166, 0.04172766, 0.04889869, 0.04546082, 0.04324546,\n",
       "       0.02149492, 0.04350594, 0.04309529, 0.05481654, 0.04427156,\n",
       "       0.03466253, 0.04352433, 0.03739424, 0.03461071, 0.02528962,\n",
       "       0.06241968, 0.05976171, 0.0548403 , 0.03217554, 0.04309573,\n",
       "       0.0312563 , 0.03491853, 0.04664799, 0.06667203, 0.04005707,\n",
       "       0.04428276, 0.05333639, 0.03911079, 0.03963643, 0.03176307,\n",
       "       0.06048664, 0.04359751, 0.03923621, 0.03896499, 0.0313111 ,\n",
       "       0.04808619, 0.04131375, 0.02316755, 0.03421379, 0.05989702,\n",
       "       0.03703154, 0.03529202, 0.04271222, 0.03769064, 0.06237305,\n",
       "       0.04668351, 0.04383942, 0.0417257 , 0.03369041, 0.0302243 ,\n",
       "       0.03521875, 0.05561442, 0.03989624, 0.05895447, 0.0633259 ,\n",
       "       0.05019855, 0.04016217, 0.03953727, 0.04215639, 0.04162873,\n",
       "       0.03927387, 0.04692849, 0.04306279, 0.03759022, 0.03441121,\n",
       "       0.04614177, 0.03635581, 0.03896729, 0.03957003, 0.06525524,\n",
       "       0.04834192, 0.04306366, 0.046015  , 0.02939674, 0.03530382,\n",
       "       0.04301154, 0.03138097, 0.06470039, 0.0494113 , 0.03518577,\n",
       "       0.0464005 , 0.0386475 , 0.03098751, 0.04644169, 0.03828277,\n",
       "       0.04300364, 0.03844452, 0.04207546, 0.05722106, 0.03592036,\n",
       "       0.02954735, 0.03808772, 0.03559735, 0.03578063, 0.03590935,\n",
       "       0.05017077, 0.04761073, 0.03338434, 0.04590691, 0.03588794,\n",
       "       0.04586808, 0.03805576, 0.04576987, 0.03446219, 0.05070699,\n",
       "       0.03412881, 0.03304023, 0.04521362, 0.04574401, 0.03800137,\n",
       "       0.04493496, 0.04550031, 0.03464963, 0.05804196, 0.06145665,\n",
       "       0.06462447, 0.04799424, 0.04023198, 0.05356713, 0.03554416,\n",
       "       0.03836807, 0.05899651, 0.02813332, 0.0387772 , 0.03050596,\n",
       "       0.04253469, 0.06013659, 0.0593553 , 0.04051169, 0.04017007,\n",
       "       0.03747278, 0.02924272, 0.03825984, 0.03207491, 0.04371831,\n",
       "       0.05667091, 0.04471146, 0.05625248, 0.04180762, 0.03085396,\n",
       "       0.04298933, 0.03492057, 0.04327037, 0.03883696, 0.04975815,\n",
       "       0.04351324, 0.03974174, 0.04282708, 0.03902674, 0.03749786,\n",
       "       0.03035677, 0.04217235, 0.0291376 , 0.05968426, 0.0543974 ,\n",
       "       0.03626222, 0.04971678, 0.03081309, 0.06507863, 0.05743801,\n",
       "       0.04118675, 0.04020021, 0.04344378, 0.03623427, 0.0310909 ,\n",
       "       0.04362853, 0.04512224, 0.0424116 , 0.02823511, 0.05942085,\n",
       "       0.04209673, 0.04562952, 0.06196681, 0.03811463, 0.04142857,\n",
       "       0.03811804, 0.04208013, 0.03395328, 0.05849946, 0.03653537,\n",
       "       0.02574361, 0.0482071 , 0.03801917, 0.057162  , 0.04016993,\n",
       "       0.03714106, 0.04233507, 0.04251983, 0.03890828, 0.04049243,\n",
       "       0.0428628 , 0.04880889, 0.02949807, 0.05874525, 0.04393202,\n",
       "       0.04552892, 0.05912535, 0.03847274, 0.03163589, 0.03676926,\n",
       "       0.04065782, 0.04971823, 0.03861944, 0.04294073, 0.04786171,\n",
       "       0.04205436, 0.04324441, 0.04295004, 0.04341553, 0.03961728,\n",
       "       0.03264315, 0.05051079, 0.0436201 , 0.03624873, 0.03942651,\n",
       "       0.03225679, 0.04433547, 0.04765609, 0.0428791 , 0.03630602,\n",
       "       0.04822715, 0.03054374, 0.03710845, 0.03705258, 0.03170795,\n",
       "       0.04879493, 0.04114094, 0.05411546, 0.03471716, 0.0378863 ,\n",
       "       0.0311664 , 0.04560418, 0.06893094, 0.0308524 , 0.03766206,\n",
       "       0.03857873, 0.03846633, 0.04062873, 0.04099519, 0.04367864,\n",
       "       0.03803883, 0.02914158, 0.04258051, 0.04655405, 0.03965151,\n",
       "       0.04869364, 0.05672323, 0.04157268, 0.05176169, 0.03922066,\n",
       "       0.04265853, 0.03570573, 0.04290523, 0.04401348, 0.05366101,\n",
       "       0.04130116, 0.04337836, 0.04273595, 0.03267458, 0.03904068,\n",
       "       0.02663089, 0.03720663, 0.04023313, 0.04243626, 0.03307895,\n",
       "       0.04124274, 0.05622328, 0.03402259, 0.04349444, 0.06865422,\n",
       "       0.02181378, 0.03882859, 0.04090069, 0.03731989, 0.03676308,\n",
       "       0.04286806, 0.03665211, 0.03746919, 0.04481272, 0.0600065 ,\n",
       "       0.04758386, 0.04331689, 0.05739835, 0.05313732, 0.03500886,\n",
       "       0.04522865, 0.03040659, 0.0358033 , 0.04711916, 0.03309201,\n",
       "       0.05934136, 0.03296162, 0.03637555, 0.03952026, 0.05169961,\n",
       "       0.0418651 , 0.03661225, 0.05131033, 0.03097814, 0.04617192,\n",
       "       0.05667091, 0.06253157, 0.04472273, 0.05246707, 0.03755096,\n",
       "       0.03662417, 0.03133152, 0.03792954, 0.05801019, 0.04973103,\n",
       "       0.054115  , 0.0449855 , 0.04885902, 0.03634097, 0.03161587,\n",
       "       0.04247044, 0.03315787, 0.03468079, 0.04161821, 0.04405556,\n",
       "       0.0469498 , 0.04003526, 0.03940322, 0.03649728, 0.03831994,\n",
       "       0.0419407 , 0.03925826, 0.03499427, 0.03676809, 0.03289524,\n",
       "       0.04170272, 0.04654041, 0.03696836, 0.05250569, 0.03617766,\n",
       "       0.06711358, 0.03561018, 0.03117079, 0.06147527, 0.0352003 ,\n",
       "       0.05123535, 0.02123687, 0.03169739, 0.03112705, 0.04165629,\n",
       "       0.05132653, 0.03444953, 0.03758042, 0.03686062, 0.05278018,\n",
       "       0.04107818, 0.0378103 , 0.0404471 , 0.05026193, 0.03998259,\n",
       "       0.04460616, 0.03662999, 0.04194839, 0.05289324, 0.02916389,\n",
       "       0.03577839, 0.03133686, 0.03578755, 0.04263325, 0.04306334,\n",
       "       0.03208916, 0.03168578, 0.039174  , 0.04288604, 0.03942429,\n",
       "       0.03181941, 0.03983361, 0.04876249, 0.0317478 , 0.03193008,\n",
       "       0.0429346 , 0.04324143, 0.03158941, 0.02195679, 0.0463701 ,\n",
       "       0.04749365, 0.04834982, 0.03293977, 0.04334137, 0.039535  ,\n",
       "       0.04759072, 0.03979385, 0.0402313 , 0.04827828, 0.03948332,\n",
       "       0.04441781, 0.04759025, 0.04666803, 0.03395606, 0.04318406,\n",
       "       0.05745796, 0.03388548, 0.03701885, 0.04361701, 0.03982589,\n",
       "       0.04185352, 0.04536727, 0.05366644, 0.05829905, 0.03526241,\n",
       "       0.03208043, 0.04109135, 0.03085237, 0.04188295, 0.04742509,\n",
       "       0.04508259, 0.0585818 , 0.06604695, 0.03578202, 0.06138628,\n",
       "       0.03829014, 0.04882602, 0.0530325 , 0.03270579, 0.04771115,\n",
       "       0.04748243, 0.03689919, 0.04865261, 0.04468184, 0.04355469,\n",
       "       0.04300566, 0.04017968, 0.03915232, 0.03585472, 0.03990321,\n",
       "       0.03979518, 0.06201562, 0.04330827, 0.02981693, 0.03048265,\n",
       "       0.04248977, 0.03822059, 0.0451711 , 0.0414663 , 0.04248617,\n",
       "       0.03841124, 0.04232275, 0.04235048, 0.04341882, 0.0536906 ,\n",
       "       0.03909931, 0.0417472 , 0.06000362, 0.04616804, 0.04308205,\n",
       "       0.04915547, 0.04463018, 0.03453011, 0.04066821, 0.03874514,\n",
       "       0.03261043, 0.03543007, 0.03808267, 0.03262949, 0.03899463,\n",
       "       0.03123361, 0.04224914, 0.03552503, 0.03229529, 0.04222816,\n",
       "       0.03742572, 0.03121767, 0.04326458, 0.0449677 , 0.037804  ,\n",
       "       0.04384253, 0.03267239, 0.04440632, 0.04314733, 0.04526459,\n",
       "       0.06809033, 0.03676604, 0.04349753, 0.05278403, 0.03829719,\n",
       "       0.04490268, 0.04346859, 0.03397281, 0.0414793 , 0.05461495,\n",
       "       0.04445993, 0.05099534, 0.03314394, 0.05194625, 0.03417696,\n",
       "       0.03466931, 0.04555021, 0.04618135, 0.05054817, 0.04095603,\n",
       "       0.0535754 , 0.03749608, 0.06622309, 0.03833637, 0.04264518,\n",
       "       0.03398975, 0.03676891, 0.04295748, 0.04376713, 0.03265039,\n",
       "       0.03491445, 0.03899154, 0.02905975, 0.06377543, 0.04981145,\n",
       "       0.04386145, 0.03379593, 0.0328035 , 0.03493078, 0.04472501,\n",
       "       0.04474588, 0.03674063, 0.03800414, 0.06213627, 0.03835763,\n",
       "       0.0522844 , 0.03686128, 0.0506633 , 0.03383693, 0.05706336,\n",
       "       0.049119  , 0.04677377, 0.02972024, 0.04307806, 0.04071171,\n",
       "       0.04104533, 0.04445356, 0.0478511 , 0.03568079, 0.04228276,\n",
       "       0.05155686, 0.05055087, 0.05793291, 0.04147904, 0.03312992,\n",
       "       0.03320251, 0.05942075, 0.03274792, 0.03493562, 0.03105145,\n",
       "       0.03224861, 0.05804059, 0.04794276, 0.03339043, 0.04720464,\n",
       "       0.03441095, 0.04017429, 0.04146736, 0.04306562, 0.04050301,\n",
       "       0.04302895, 0.04792125, 0.03253812, 0.04162073, 0.04903273,\n",
       "       0.03867025, 0.06628562, 0.03535005, 0.03599719, 0.03011558,\n",
       "       0.03282022, 0.02246654, 0.0402411 , 0.05126487, 0.03397065,\n",
       "       0.04218233, 0.05070363, 0.02908178, 0.02050345, 0.06318539,\n",
       "       0.03533868, 0.0387287 , 0.05772331, 0.03750263, 0.04232784,\n",
       "       0.03003488, 0.0502877 , 0.04205904, 0.03212616, 0.04536562,\n",
       "       0.05859067, 0.03303964, 0.05093236, 0.04213295, 0.05718896,\n",
       "       0.04248994, 0.03916853, 0.03848981, 0.02779638, 0.05049348,\n",
       "       0.04171467, 0.03853544, 0.04563811, 0.03129636, 0.02939869,\n",
       "       0.04328105, 0.04938978, 0.04296163, 0.02901604, 0.03917964,\n",
       "       0.03875708, 0.04301814, 0.02829385, 0.06894748, 0.03747176,\n",
       "       0.04610565, 0.03612291, 0.04295797, 0.06261206, 0.03549334,\n",
       "       0.05397871, 0.04155676, 0.04755111, 0.04071955, 0.0404247 ,\n",
       "       0.04043754, 0.03214966, 0.0477087 , 0.04386204, 0.04352972,\n",
       "       0.03881156, 0.04734054, 0.03134491, 0.05969694, 0.04139611,\n",
       "       0.04612242, 0.03427258, 0.04624758, 0.04815064, 0.04135327,\n",
       "       0.03594785, 0.04648274, 0.0537952 , 0.04004364, 0.04782243,\n",
       "       0.04423413, 0.04538313, 0.06108555, 0.04513679, 0.0390198 ,\n",
       "       0.03921483, 0.03884034, 0.04237734, 0.0379281 , 0.03961824,\n",
       "       0.03534895, 0.03892072, 0.03867371, 0.04042959, 0.043351  ,\n",
       "       0.05113342, 0.06445412, 0.03684329, 0.05942343, 0.029804  ,\n",
       "       0.03819822, 0.04197083, 0.04163807, 0.03314558, 0.0638557 ,\n",
       "       0.03173716, 0.03923722, 0.04726989, 0.03980201, 0.04180733,\n",
       "       0.04842917, 0.04827375, 0.04110407, 0.02948483, 0.03081716,\n",
       "       0.0604616 , 0.03326097, 0.04740743, 0.03678494, 0.04240741,\n",
       "       0.03741496, 0.05170198, 0.03551415, 0.02307041, 0.04216553,\n",
       "       0.03513969, 0.03112871, 0.05063033, 0.03195665, 0.04362766,\n",
       "       0.0438324 , 0.05113996, 0.04476326, 0.03138555, 0.0373444 ,\n",
       "       0.04789227, 0.03428232, 0.06387059, 0.03975992, 0.04113065,\n",
       "       0.03484219, 0.03839708, 0.03906749, 0.04254929, 0.04862363,\n",
       "       0.03679699, 0.04318427, 0.05206386, 0.06102719, 0.0391055 ,\n",
       "       0.03590371, 0.03395022, 0.04179788, 0.02890629, 0.04277801,\n",
       "       0.03117914, 0.03578387, 0.04286114, 0.03598286, 0.03552841,\n",
       "       0.04522615, 0.03313976, 0.03643253, 0.06716292, 0.04658301,\n",
       "       0.06675129, 0.03471912, 0.04405932, 0.05883591, 0.04622913,\n",
       "       0.04449429, 0.05392865, 0.03641316, 0.05749516, 0.0495789 ,\n",
       "       0.04793206, 0.05423594, 0.05195983, 0.04234895, 0.06523136,\n",
       "       0.02484471, 0.03474326, 0.03455612, 0.04702672, 0.04942206,\n",
       "       0.04573217, 0.03634835, 0.03371516, 0.04054642, 0.03865349,\n",
       "       0.03497029, 0.04387637, 0.04284291, 0.05316076, 0.03926798],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_obj(pred, \"../pred_loss_Xx.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
