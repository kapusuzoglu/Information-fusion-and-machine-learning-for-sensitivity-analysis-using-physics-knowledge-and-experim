{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "class InputTransformedKernel(tfp.math.psd_kernels.PositiveSemidefiniteKernel):\n",
    "\n",
    "    def __init__(self, kernel, transformation, name='InputTransformedKernel'):\n",
    "        self._kernel = kernel\n",
    "        self._transformation = transformation\n",
    "        super(InputTransformedKernel, self).__init__(\n",
    "            feature_ndims=kernel.feature_ndims,\n",
    "            dtype=kernel.dtype,\n",
    "            name=name)\n",
    "\n",
    "    def apply(self, x1, x2):\n",
    "        return self._kernel.apply(\n",
    "            self._transformation(x1),\n",
    "            self._transformation(x2))\n",
    "\n",
    "    def matrix(self, x1, x2):\n",
    "        return self._kernel.matrix(\n",
    "            self._transformation(x1),\n",
    "            self._transformation(x2))\n",
    "\n",
    "    @property\n",
    "    def batch_shape(self):\n",
    "        return self._kernel.batch_shape\n",
    "\n",
    "    def batch_shape_tensor(self):\n",
    "        return self._kernel.batch_shape_tensor\n",
    "\n",
    "class InputScaledKernel(InputTransformedKernel):\n",
    "\n",
    "    def __init__(self, kernel, length_scales):\n",
    "        super(InputScaledKernel, self).__init__(\n",
    "            kernel,\n",
    "            lambda x: x / tf.expand_dims(length_scales,\n",
    "                                     -(kernel.feature_ndims + 1)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled data\n",
    "data = np.loadtxt('../data/labeled_data.dat')\n",
    "x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "# Normalize the data.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# normalize dataset with MinMaxScaler\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "x_labeled = scaler.fit_transform(x_labeled)\n",
    "# y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "tr_size = 30\n",
    "\n",
    "# train and test data\n",
    "trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "trainY = np.transpose(trainY)\n",
    "testY = np.transpose(testY)\n",
    "\n",
    "data_phyloss = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "x_unlabeled = data_phyloss[:, :]\n",
    "\n",
    "# initial porosity\n",
    "initporo = x_unlabeled[:, -1]\n",
    "\n",
    "x_unlabeled1 = x_unlabeled[:1303, :2]\n",
    "x_unlabeled2 = x_unlabeled[-6:, :2]\n",
    "x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "init_poro1 = initporo[:1303]\n",
    "init_poro2 = initporo[-6:]\n",
    "init_poro = np.hstack((init_poro1,init_poro2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gp(amplitude, length_scale):\n",
    "    \"\"\"Defines the conditional dist. of GP outputs, given kernel parameters.\"\"\"\n",
    "\n",
    "    # Create the covariance kernel, which will be shared between the prior (which we\n",
    "    # use for maximum likelihood training) and the posterior (which we use for\n",
    "    # posterior predictive sampling)    \n",
    "    se_kernel = tfk.ExponentiatedQuadratic(amplitude)  # length_scale = None here, implicitly\n",
    "\n",
    "    # This is the \"ARD\" kernel (we don't like abbreviations or bizarrely obscure names in\n",
    "    # TFP, so we're probably going to call this \"InputScaledKernel\" since....that's what it is! :)\n",
    "    kernel = InputScaledKernel(se_kernel, length_scale)\n",
    "    \n",
    "    # Create the GP prior distribution, which we will use to train the model\n",
    "    # parameters.\n",
    "    return tfd.GaussianProcess(kernel=kernel,index_points=trainX)\n",
    "\n",
    "gp_joint_model = tfd.JointDistributionNamedAutoBatched({\n",
    "    'amplitude': tfd.TransformedDistribution(\n",
    "            distribution=tfd.Normal(loc=0., scale=np.float64(1.)),\n",
    "            bijector=tfb.Exp(),\n",
    "            batch_shape=[1]),\n",
    "    'length_scale': tfd.TransformedDistribution(\n",
    "            distribution=tfd.Normal(loc=0., scale=np.float64(1.)),\n",
    "            bijector=tfb.Exp(),\n",
    "            batch_shape=[2]),\n",
    "    'observations': build_gp,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainable model parameters, which we'll subsequently optimize.\n",
    "# Note that we constrain them to be strictly positive.\n",
    "constrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())\n",
    "\n",
    "amplitude_var = tfp.util.TransformedVariable(\n",
    "    initial_value=np.random.uniform(size=1),\n",
    "    bijector=constrain_positive,\n",
    "    name='amplitude',\n",
    "    dtype=np.float64)\n",
    "\n",
    "length_scale_var = tfp.util.TransformedVariable(\n",
    "    initial_value=np.random.uniform(size=[2]),\n",
    "    bijector=constrain_positive,\n",
    "    name='length_scale',\n",
    "    dtype=np.float64)\n",
    "\n",
    "trainable_variables = [v.trainable_variables[0] for v in \n",
    "                       [amplitude_var,\n",
    "                       length_scale_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(autograph=False, experimental_compile=False)\n",
    "def target_log_prob(amplitude, length_scale, poroi, lam):\n",
    "    tf.random.set_seed(1234)\n",
    "    se_kernel = tfk.ExponentiatedQuadratic(amplitude)  # length_scale = None here, implicitly\n",
    "    optimized_kernel = InputScaledKernel(se_kernel, length_scale)\n",
    "    gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = x_unlabeled)\n",
    "    samples = gprm.sample(1)\n",
    "    pred = tf.squeeze(samples, axis=0)\n",
    "\n",
    "    phyloss_poro = tf.math.reduce_mean(tf.nn.relu(tf.negative(pred))+tf.nn.relu(pred-poroi))\n",
    "\n",
    "#     print(\"phyloss_poro:\",lam*phyloss_poro)\n",
    "#     return lam*phyloss_poro\n",
    "    return lam*phyloss_poro - gp_joint_model.log_prob({\n",
    "      'amplitude': amplitude,\n",
    "      'length_scale': length_scale,\n",
    "      'observations': trainY\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss_inloop: tf.Tensor(97327.17451122966, shape=(), dtype=float64)\n",
      "1 loss_inloop: tf.Tensor(27517.653673109533, shape=(), dtype=float64)\n",
      "2 loss_inloop: tf.Tensor(26125.867318869172, shape=(), dtype=float64)\n",
      "3 loss_inloop: tf.Tensor(16993.676754054417, shape=(), dtype=float64)\n",
      "4 loss_inloop: tf.Tensor(63578.97302255117, shape=(), dtype=float64)\n",
      "5 loss_inloop: tf.Tensor(3150.835495812226, shape=(), dtype=float64)\n",
      "6 loss_inloop: tf.Tensor(23689.326876749325, shape=(), dtype=float64)\n",
      "7 loss_inloop: tf.Tensor(26075.86432005829, shape=(), dtype=float64)\n",
      "8 loss_inloop: tf.Tensor(33741.71708333892, shape=(), dtype=float64)\n",
      "9 loss_inloop: tf.Tensor(35355.67059762244, shape=(), dtype=float64)\n",
      "10 loss_inloop: tf.Tensor(17614.334487859152, shape=(), dtype=float64)\n",
      "11 loss_inloop: tf.Tensor(43646.772000166406, shape=(), dtype=float64)\n",
      "12 loss_inloop: tf.Tensor(14456.26112218793, shape=(), dtype=float64)\n",
      "13 loss_inloop: tf.Tensor(19869.55961840706, shape=(), dtype=float64)\n",
      "14 loss_inloop: tf.Tensor(27157.113070819923, shape=(), dtype=float64)\n",
      "15 loss_inloop: tf.Tensor(22702.13329835835, shape=(), dtype=float64)\n",
      "16 loss_inloop: tf.Tensor(3567.7264449614204, shape=(), dtype=float64)\n",
      "17 loss_inloop: tf.Tensor(9866.976109664196, shape=(), dtype=float64)\n",
      "18 loss_inloop: tf.Tensor(19225.984733324294, shape=(), dtype=float64)\n",
      "19 loss_inloop: tf.Tensor(13203.337507448356, shape=(), dtype=float64)\n",
      "20 loss_inloop: tf.Tensor(1293.67803109731, shape=(), dtype=float64)\n",
      "21 loss_inloop: tf.Tensor(13518.000383230443, shape=(), dtype=float64)\n",
      "22 loss_inloop: tf.Tensor(5009.1633233610955, shape=(), dtype=float64)\n",
      "23 loss_inloop: tf.Tensor(277.64999388241733, shape=(), dtype=float64)\n",
      "24 loss_inloop: tf.Tensor(7825.713764976132, shape=(), dtype=float64)\n",
      "25 loss_inloop: tf.Tensor(1759.1652639855802, shape=(), dtype=float64)\n",
      "26 loss_inloop: tf.Tensor(14773.369412708786, shape=(), dtype=float64)\n",
      "27 loss_inloop: tf.Tensor(1765.7540772545804, shape=(), dtype=float64)\n",
      "28 loss_inloop: tf.Tensor(5333.5747779841795, shape=(), dtype=float64)\n",
      "29 loss_inloop: tf.Tensor(6760.073662466977, shape=(), dtype=float64)\n",
      "30 loss_inloop: tf.Tensor(1512.9215296156717, shape=(), dtype=float64)\n",
      "31 loss_inloop: tf.Tensor(2654.971829314036, shape=(), dtype=float64)\n",
      "32 loss_inloop: tf.Tensor(4082.207350195424, shape=(), dtype=float64)\n",
      "33 loss_inloop: tf.Tensor(5899.132830379488, shape=(), dtype=float64)\n",
      "34 loss_inloop: tf.Tensor(305.5257969773975, shape=(), dtype=float64)\n",
      "35 loss_inloop: tf.Tensor(2774.96189281477, shape=(), dtype=float64)\n",
      "36 loss_inloop: tf.Tensor(4672.3478895588105, shape=(), dtype=float64)\n",
      "37 loss_inloop: tf.Tensor(910.9701442382734, shape=(), dtype=float64)\n",
      "38 loss_inloop: tf.Tensor(4687.600505674522, shape=(), dtype=float64)\n",
      "39 loss_inloop: tf.Tensor(1024.8677256371059, shape=(), dtype=float64)\n",
      "Trained parameters:\n",
      "amplitude: [0.07632151]\n",
      "length_scale: [0.76986674 0.18299228]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Optimize the model parameters.\n",
    "num_iters = 40\n",
    "lam = 100000\n",
    "optimizer = tf.optimizers.Adam(learning_rate=.1)\n",
    "\n",
    "# Store the likelihood values during training, so we can plot the progress\n",
    "lls_ = np.zeros(num_iters, np.float64)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = target_log_prob(amplitude_var, length_scale_var, init_poro, lam) # physics loss & normal loss\n",
    "\n",
    "\n",
    "    print(i,\"loss_inloop:\",loss)\n",
    "    grads = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    lls_[i] = loss\n",
    "\n",
    "print('Trained parameters:')\n",
    "print('amplitude: {}'.format(amplitude_var._value().numpy()))\n",
    "print('length_scale: {}'.format(length_scale_var._value().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def pass_arg(Xx, nsim, tr_size):\n",
    "    print(\"Tr_size:\", tr_size)\n",
    "    def fix_seeds(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    #     K.set_session(sess)\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    ss = 1\n",
    "    fix_seeds(ss)\n",
    "\n",
    "    # Compute the RMSE given the ground truth (y_true) and the predictions(y_pred)\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "            return tf.math.sqrt(tf.math.reduce_mean(tf.math.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "    class InputTransformedKernel(tfp.math.psd_kernels.PositiveSemidefiniteKernel):\n",
    "\n",
    "        def __init__(self, kernel, transformation, name='InputTransformedKernel'):\n",
    "            self._kernel = kernel\n",
    "            self._transformation = transformation\n",
    "            super(InputTransformedKernel, self).__init__(\n",
    "                feature_ndims=kernel.feature_ndims,\n",
    "                dtype=kernel.dtype,\n",
    "                name=name)\n",
    "\n",
    "        def apply(self, x1, x2):\n",
    "            return self._kernel.apply(\n",
    "                self._transformation(x1),\n",
    "                self._transformation(x2))\n",
    "\n",
    "        def matrix(self, x1, x2):\n",
    "            return self._kernel.matrix(\n",
    "                self._transformation(x1),\n",
    "                self._transformation(x2))\n",
    "\n",
    "        @property\n",
    "        def batch_shape(self):\n",
    "            return self._kernel.batch_shape\n",
    "\n",
    "        def batch_shape_tensor(self):\n",
    "            return self._kernel.batch_shape_tensor\n",
    "\n",
    "    class InputScaledKernel(InputTransformedKernel):\n",
    "\n",
    "        def __init__(self, kernel, length_scales):\n",
    "            super(InputScaledKernel, self).__init__(\n",
    "                kernel,\n",
    "                lambda x: x / tf.expand_dims(length_scales,\n",
    "                                         -(kernel.feature_ndims + 1)))\n",
    "\n",
    "    # Load labeled data\n",
    "    data = np.loadtxt('../data/labeled_data.dat')\n",
    "    x_labeled = data[:, :2].astype(np.float64) # -2 because we do not need porosity predictions\n",
    "    y_labeled = data[:, -2:-1].astype(np.float64) # dimensionless bond length and porosity measurements\n",
    "\n",
    "    # Normalize the data.\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    # normalize dataset with MinMaxScaler\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    x_labeled = scaler.fit_transform(x_labeled)\n",
    "    # y_labeled = scaler.fit_transform(y_labeled)\n",
    "\n",
    "    tr_size = 30\n",
    "\n",
    "    # train and test data\n",
    "    trainX, trainY = x_labeled[:tr_size,:], y_labeled[:tr_size]\n",
    "    testX, testY = x_labeled[tr_size:,:], y_labeled[tr_size:]\n",
    "\n",
    "    trainY = np.transpose(trainY)\n",
    "    testY = np.transpose(testY)\n",
    "\n",
    "    data_phyloss = np.loadtxt('../data/unlabeled_data_BK_constw_v2_1525.dat')\n",
    "    x_unlabeled = data_phyloss[:, :]\n",
    "\n",
    "    # initial porosity\n",
    "    initporo = x_unlabeled[:, -1]\n",
    "\n",
    "    x_unlabeled1 = x_unlabeled[:1303, :2]\n",
    "    x_unlabeled2 = x_unlabeled[-6:, :2]\n",
    "    x_unlabeled = np.vstack((x_unlabeled1,x_unlabeled2))\n",
    "\n",
    "    x_unlabeled = scaler.fit_transform(x_unlabeled)\n",
    "    init_poro1 = initporo[:1303]\n",
    "    init_poro2 = initporo[-6:]\n",
    "    init_poro = np.hstack((init_poro1,init_poro2))\n",
    "\n",
    "\n",
    "    def build_gp(amplitude, length_scale):\n",
    "        \"\"\"Defines the conditional dist. of GP outputs, given kernel parameters.\"\"\"\n",
    "\n",
    "        # Create the covariance kernel, which will be shared between the prior (which we\n",
    "        # use for maximum likelihood training) and the posterior (which we use for\n",
    "        # posterior predictive sampling)    \n",
    "        se_kernel = tfk.ExponentiatedQuadratic(amplitude)  # length_scale = None here, implicitly\n",
    "\n",
    "        # This is the \"ARD\" kernel (we don't like abbreviations or bizarrely obscure names in\n",
    "        # TFP, so we're probably going to call this \"InputScaledKernel\" since....that's what it is! :)\n",
    "        kernel = InputScaledKernel(se_kernel, length_scale)\n",
    "\n",
    "        # Create the GP prior distribution, which we will use to train the model\n",
    "        # parameters.\n",
    "        return tfd.GaussianProcess(kernel=kernel,index_points=trainX)\n",
    "\n",
    "    gp_joint_model = tfd.JointDistributionNamedAutoBatched({\n",
    "        'amplitude': tfd.TransformedDistribution(\n",
    "                distribution=tfd.Normal(loc=0., scale=np.float64(1.)),\n",
    "                bijector=tfb.Exp(),\n",
    "                batch_shape=[1]),\n",
    "        'length_scale': tfd.TransformedDistribution(\n",
    "                distribution=tfd.Normal(loc=0., scale=np.float64(1.)),\n",
    "                bijector=tfb.Exp(),\n",
    "                batch_shape=[2]),\n",
    "        'observations': build_gp,\n",
    "    })\t\t\t\t\t\n",
    "\n",
    "\n",
    "\n",
    "    # Create the trainable model parameters, which we'll subsequently optimize.\n",
    "    # Note that we constrain them to be strictly positive.\n",
    "    constrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())\n",
    "\n",
    "    amplitude_var = tfp.util.TransformedVariable(\n",
    "        initial_value=np.random.uniform(size=1),\n",
    "        bijector=constrain_positive,\n",
    "        name='amplitude',\n",
    "        dtype=np.float64)\n",
    "\n",
    "    length_scale_var = tfp.util.TransformedVariable(\n",
    "        initial_value=np.random.uniform(size=[2]),\n",
    "        bijector=constrain_positive,\n",
    "        name='length_scale',\n",
    "        dtype=np.float64)\n",
    "\n",
    "    trainable_variables = [v.trainable_variables[0] for v in \n",
    "                           [amplitude_var,\n",
    "                           length_scale_var]]\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function(autograph=False, experimental_compile=False)\n",
    "    def target_log_prob(amplitude, length_scale, poroi, lam):\n",
    "        tf.random.set_seed(1234)\n",
    "        se_kernel = tfk.ExponentiatedQuadratic(amplitude)  # length_scale = None here, implicitly\n",
    "        optimized_kernel = InputScaledKernel(se_kernel, length_scale)\n",
    "        gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = x_unlabeled)\n",
    "        samples = gprm.sample(1)\n",
    "        pred = tf.squeeze(samples, axis=0)\n",
    "\n",
    "        phyloss_poro = tf.math.reduce_mean(tf.nn.relu(tf.negative(pred))+tf.nn.relu(pred-poroi))\n",
    "\n",
    "    #     print(\"phyloss_poro:\",lam*phyloss_poro)\n",
    "    #     return lam*phyloss_poro\n",
    "        return lam*phyloss_poro - gp_joint_model.log_prob({\n",
    "          'amplitude': amplitude,\n",
    "          'length_scale': length_scale,\n",
    "          'observations': trainY\n",
    "      })\n",
    "\n",
    "\n",
    "    tf.random.set_seed(1234)\n",
    "\n",
    "    # Optimize the model parameters.\n",
    "    num_iters = 40\n",
    "    lam = 100000\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=.1)\n",
    "\n",
    "    # Store the likelihood values during training, so we can plot the progress\n",
    "    lls_ = np.zeros(num_iters, np.float64)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = target_log_prob(amplitude_var, length_scale_var, init_poro, lam) # physics loss & normal loss\n",
    "\n",
    "\n",
    "        # print(i,\"loss_inloop:\",loss)\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        lls_[i] = loss\n",
    "\n",
    "    # print('Trained parameters:')\n",
    "    # print('amplitude: {}'.format(amplitude_var._value().numpy()))\n",
    "    # print('length_scale: {}'.format(length_scale_var._value().numpy()))\n",
    "\n",
    "\n",
    "\n",
    "    tf.random.set_seed(1234)\n",
    "    se_kernel = tfk.ExponentiatedQuadratic(amplitude_var)  # length_scale = None here, implicitly\n",
    "    optimized_kernel = InputScaledKernel(se_kernel, length_scale_var)\n",
    "    gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = testX)\n",
    "    samples = gprm.sample(1)\n",
    "    samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 9), dtype=float64, numpy=\n",
       "array([[[-0.09655712, -0.09761954, -0.06867765,  0.08486226,\n",
       "          0.07861124,  0.11342095,  0.09207502, -0.07753923,\n",
       "          0.03311804]],\n",
       "\n",
       "       [[ 0.04212455,  0.03606457,  0.06086621,  0.0506872 ,\n",
       "         -0.11496251, -0.09564325, -0.02192549,  0.0386103 ,\n",
       "          0.02477364]]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "se_kernel = tfk.ExponentiatedQuadratic(amplitude_var)  # length_scale = None here, implicitly\n",
    "optimized_kernel = InputScaledKernel(se_kernel, length_scale_var)\n",
    "gprm = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = testX)\n",
    "samples = gprm.sample(2)\n",
    "# optimized_kernel = tfk.ExponentiatedQuadratic(amplitude_var, length_scale_var)\n",
    "# gpr = tfd.GaussianProcessRegressionModel(kernel=optimized_kernel, index_points = testX)\n",
    "# samples = gpr.sample(1)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf.squeeze(samples, axis=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0105933 , 0.03315482, 0.01352198, 0.05269549, 0.00490424,\n",
       "        0.00895544, 0.01003564, 0.01345683, 0.02496436]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.08679325]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_mean_squared_error(testY, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c555797ef780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Plot the loss evolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlls_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training iteration\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the loss evolution\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(lls_)\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Log marginal likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
